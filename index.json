[{"authors":["admin"],"categories":null,"content":"Hi, I\u0026rsquo;m Alok! I\u0026rsquo;m a 3rd-year CS Ph.D. student at UC Berkeley, advised by Aydın Buluç and Kathy Yelick. I am affiliated with the PASSION Lab, the BeBOp Group, and Lawrence Berkeley National Lab.\nVery broadly, I want to discover what scientific questions we can answer if computational performance were not an issue. To that end, I work on accelerating scientific computation on large-scale supercomputers. I am currently interested in scaling graph-representation learning, with applications to bioinformatics. In the past, I worked on accelerating graph analytics on GPUs. My work is supported by the NSF Fellowship.\nI am also passionate about science communication and write regularly for the Berkeley Science Review.\nBefore coming to Berkeley, I worked with Oded Green in David Bader\u0026rsquo;s HPC lab at Georgia Tech (Go Jackets!).\n","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"598b63dd58b43bce02403646f240cd3c","permalink":"/author/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/admin/","section":"author","summary":"Hi, I\u0026rsquo;m Alok! I\u0026rsquo;m a 3rd-year CS Ph.D. student at UC Berkeley, advised by Aydın Buluç and Kathy Yelick. I am affiliated with the PASSION Lab, the BeBOp Group, and Lawrence Berkeley National Lab.\nVery broadly, I want to discover what scientific questions we can answer if computational performance were not an issue. To that end, I work on accelerating scientific computation on large-scale supercomputers. I am currently interested in scaling graph-representation learning, with applications to bioinformatics.","tags":null,"title":"Alok Tripathy","type":"author"},{"authors":["Oguz Selvitopi","Benjamin Brock","Israt Nisa","Alok Tripathy","Katherine Yelick","Aydın Buluç"],"categories":[],"content":"","date":1623264214,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1623264214,"objectID":"897ba7655f24870ebc73613b0ad85b25","permalink":"/publication/2021-ics-dspmm/","publishdate":"2021-06-09T14:43:34-04:00","relpermalink":"/publication/2021-ics-dspmm/","section":"publication","summary":"Sparse times dense matrix multiplication (SpMM) finds its applications in well-established fields such as computational linear algebra as well as emerging fields such as graph neural networks. In this study, we evaluate the performance of various techniques for performing SpMM as a distributed computation across many nodes by focusing on GPU accelerators. We examine how the actual local computational performance of state-of-the-art SpMM implementations affect computational efficiency as dimensions change when we scale to large numbers of nodes, which proves to be an unexpectedly important bottleneck. We consider various distribution strategies, including A-Stationary, B-Stationary, and C-Stationary algorithms, 1.5D and 2D algorithms, and RDMA-based and bulk synchronous methods of data transfer. Our results show that the best choice of algorithm and implementation technique depends not only on the cost of communication for particular matrix sizes and dimensions, but also on the performance of local SpMM operations. Our evaluations reveal that with the involvement of GPU accelerators, the best design choices for SpMM differ from the conventional algorithms that are known to perform well for dense matrix-matrix or sparse matrix-sparse matrix multiplies.","tags":["Distributed Matrix Multiplication"],"title":"Distributed-Memory Parallel Algorithms for Sparse Times Tall-Skinny-Dense Matrix Multiplication","type":"publication"},{"authors":null,"categories":[],"content":"","date":1618494600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1618494600,"objectID":"4d8122261b136ec957ca67be0c534932","permalink":"/talk/2021-gtc-mhg/","publishdate":"2021-04-15T10:00:00-03:50","relpermalink":"/talk/2021-gtc-mhg/","section":"talk","summary":"","tags":["Accelerators","Hash Tables"],"title":"Multi-GPU HashGraph: A Scalable Hash Table for NUMA Systems","type":"talk"},{"authors":null,"categories":[],"content":"","date":1618408200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1618408200,"objectID":"76c32d27e8a3a03c6829ca1479c5d86f","permalink":"/talk/2021-gtc-gnn/","publishdate":"2021-04-14T10:00:00-03:50","relpermalink":"/talk/2021-gtc-gnn/","section":"talk","summary":"","tags":["Graph-Representation Learning","Graph Neural Networks"],"title":"Reducing Communication in Graph Neural Network Training","type":"talk"},{"authors":["Alok Tripathy","Oded Green"],"categories":[],"content":"","date":1615315414,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1615315414,"objectID":"a5097f5027df4802ae2920fb2200e56d","permalink":"/publication/2021-arxiv-mhg/","publishdate":"2021-03-09T14:43:34-04:00","relpermalink":"/publication/2021-arxiv-mhg/","section":"publication","summary":"Hash tables are used in a plethora of applications, including database operations, DNA sequencing, string searching, and many more. As such, there are many parallelized hash tables targeting multicore, distributed, and accelerator-based systems. We present in this work a multi-GPU hash table implementation that can process keys at a throughput comparable to that of distributed hash tables. Distributed CPU hash tables have received significantly more attention than GPU-based hash tables. We show that a single node with multiple GPUs offers roughly the same performance as a 500-1,000-core CPU-based cluster. Our algorithm's key component is our use of multiple sparse-graph data structures and binning techniques to build the hash table. As has been shown individually, these components can be written with massive parallelism that is amenable to GPU acceleration. Since we focus on an individual node, we also leverage communication primitives that are typically prohibitive in distributed environments. We show that our new multi-GPU algorithm shares many of the same features of the single GPU algorithm -- thus we have efficient collision management capabilities and can deal with a large number of duplicates. We evaluate our algorithm on two multi-GPU compute nodes: 1) an NVIDIA DGX2 server with 16 GPUs and 2) an IBM Power 9 Processor with 6 NVIDIA GPUs. With 32-bit keys, our implementation processes 8B keys per second, comparable to some 500-1,000-core CPU-based clusters and 4X faster than prior single-GPU implementations.","tags":["GPU Data Structure"],"title":"Scalable Hash Table for NUMA Systems","type":"publication"},{"authors":null,"categories":[],"content":"","date":1607626200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607626200,"objectID":"fb97985ea77e768abd8679c09d6d007b","permalink":"/talk/2020-biggraphs/","publishdate":"2020-12-10T15:00:00-03:50","relpermalink":"/talk/2020-biggraphs/","section":"talk","summary":"","tags":["Graph Analytics"],"title":"Accurately and Efficiently Estimating Dynamic Point-to-Point Shortest Path","type":"talk"},{"authors":null,"categories":[],"content":"","date":1605725400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1605725400,"objectID":"46a871055f01ce6c7998753d79c19c26","permalink":"/talk/2020-sc/","publishdate":"2020-11-18T15:00:00-03:50","relpermalink":"/talk/2020-sc/","section":"talk","summary":"","tags":["Graph-Representation Learning","Graph Neural Networks"],"title":"Reducing Communication in Graph Neural Network Training","type":"talk"},{"authors":["Alok Tripathy","Oded Green"],"categories":[],"content":"","date":1604587096,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1604587096,"objectID":"afd6a51377ca7dba1f537269fb7a96fb","permalink":"/publication/2020-biggraphs-ppsp/","publishdate":"2020-11-05T10:38:16-04:00","relpermalink":"/publication/2020-biggraphs-ppsp/","section":"publication","summary":"Point-to-point shortest path (PPSP), or s-t connectivity, is a variant of the shortest path problem found in graph theory. In this problem, we are given a graph and pairs of vertices over time, and the output is the shortest path between each pair of vertices. In this paper, we present two algorithms. Our first algorithm approximately solves the PPSP problem on any arbitrary graph. For each pair of vertices queried, it accurately and efficiently estimates the shortest path between the two vertices. Our second algorithm extends the first to work on dynamic graphs. That is, our second algorithm can efficiently account for changes in the graph, such as friend requests on the Facebook network or road closures on road networks. At a high level, both algorithms partition the graph into highly connected communities. To respond to a query q(u, v), they each find the fewest number of partitions between u and v, and the shortest path through each partition. We show that our static graph algorithm can approximate the distance between two vertices with about 20%-35% percent error and anywhere from 80X-70000X faster than a BFS in practice with the right choice of partitions. Additionally, we show that our dynamic graph algorithm can account for updates to the graph anywhere from 20X-20000X faster than rerunning the static graph algorithm for each change to the graph.","tags":["Graph Analytics"],"title":"Accurately and Efficiently Estimating Dynamic Point-to-Point Shortest Path","type":"publication"},{"authors":["Alok Tripathy","Katherine Yelick","Aydın Buluç"],"categories":[],"content":"","date":1589049814,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1589049814,"objectID":"62202cca4ffe07b9dafa8dfc81a2f64d","permalink":"/publication/2020-sc-gnn/","publishdate":"2020-05-09T14:43:34-04:00","relpermalink":"/publication/2020-sc-gnn/","section":"publication","summary":"Graph Neural Networks (GNNs) are powerful and flexible neural networks that use the naturally sparse connectivity information of the data. GNNs represent this connectivity as sparse matrices, which have lower arithmetic intensity and thus higher communication costs compared to dense matrices, making GNNs harder to scale to high concurrencies than convolutional or fully-connected neural networks. We present a family of parallel algorithms for training GNNs. These algorithms are based on their counterparts in dense and sparse linear algebra, but they had not been previously applied to GNN training. We show that they can asymptotically reduce communication compared to existing parallel GNN training methods. We implement a promising and practical version that is based on 2D sparse-dense matrix multiplication using torch.distributed. Our implementation parallelizes over GPU-equipped clusters. We train GNNs on up to a hundred GPUs on datasets that include a protein network with over a billion edges.","tags":["Graph-Representation Learning"],"title":"Reducing Communication in Graph Neural Network Training","type":"publication"},{"authors":["James Fox","Alok Tripathy","Oded Green"],"categories":[],"content":"","date":1569437014,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569437014,"objectID":"df3e1d37a373f274633c1f3db5628c04","permalink":"/publication/2019-hpec-lrb/","publishdate":"2019-09-25T14:43:34-04:00","relpermalink":"/publication/2019-hpec-lrb/","section":"publication","summary":"Effective scheduling and load balancing of applications on massively multi-threading systems remains challenging despite decades of research, especially for irregular and data dependent problems where the execution control path is unknown until run-time. One of the most widely used loadbalancing schemes used for data dependent problems is a parallel prefix sum (PPS) array over the expected amount of work per task, followed by a partitioning of tasks to threads. While sufficient for many systems, it is not ideal for massively multithreaded systems with SIMD/SIMT execution, such as GPUs.  More fine-grained load-balancing is needed to effectively utilize SIMD/SIMT units. In this paper we introduce Logarithmic Radix Binning (LRB) as a more suitable alternative to parallel prefix summation for load-balancing on such systems. We show that LRB has better scalability than PPS for high thread counts on Intel’s Knight’s Landing processor and comparable scalability on NVIDIA Volta GPUs. On the application side, we show how LRB improves the performance of PageRank up to 1.75X using the branch-avoiding model. We also show how to better load-balance segmented sort and improve performance on the GPU.","tags":["Graph Analytics"],"title":"Improving Scheduling for Irregular Applications with Logarithmic Radix Binning","type":"publication"},{"authors":["Alok Tripathy","Fred Hohman","Duen Horng Chau","Oded Green"],"categories":[],"content":"","date":1544711896,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1544711896,"objectID":"c1062d764c6be49868edced6e80f70c9","permalink":"/publication/2018-bigdata-kcore/","publishdate":"2018-12-13T10:38:16-04:00","relpermalink":"/publication/2018-bigdata-kcore/","section":"publication","summary":"The k-core of a graph is a metric used in a wide range of applications, including social networks analytics, visualization, and graph coloring. Finding the maximal k-core of a graph can be be done in near linear time. The low computational requirements for finding the maximal k-core makes effective parallelization challenging, especially for the iterative algorithms that prune vertices and edges that no longer meet the requirements of the maximal k-core and require rebuilding the graph every iteration. In this paper, we present a new parallel and scalable algorithm for finding the maximal k-core. Similar to past algorithms, our algorithm also prunes vertices and edges. Unlike past approaches, our new algorithm does not rebuild the graph in every iteration-rather, it uses a dynamic graph data structure and avoids one of the largest performance penalties of k-core. We also show how to extend our algorithm to support k-core edge decomposition for different size k-cores found in the graph. This can be used for visualization and community analysis. While our new algorithms are architecture independent, our implementations target NVIDIA GPUs. When comparing our algorithms against several highly optimized algorithms, including the sequential igraph implementation and the multi-thread ParK implementation, our new algorithms are significantly faster. For finding the maximal k-core in the graph, our new algorithm can be up-to 58× faster the igraph and up-to 4× faster than ParK executed on a 36 core (72 thread) system. For the k-core decomposition algorithm, we saw even greater and more consistent speedups for our algorithm where it was up-to 130× faster than igraph and up-to 8× faster than ParK. Our algorithms were executed on an NVIDIA P100 GPU.","tags":["Graph Analytics"],"title":"Scalable K-Core Decomposition for Static Graphs Using a Dynamic Graph Data Structure","type":"publication"},{"authors":["Oded Green","James Fox","Alex Watkins","Alok Tripathy","Kasimir Gabert","Euna Kim","Xiaojing An","Kumar Aatish","David A. Bader"],"categories":[],"content":"","date":1537901014,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1537901014,"objectID":"63901c5c7cf92c77ece668d5fc21a44d","permalink":"/publication/2018-hpec-lrb/","publishdate":"2018-09-25T14:43:34-04:00","relpermalink":"/publication/2018-hpec-lrb/","section":"publication","summary":"Triangle counting is a building block for numerous graph applications and given the fact that graphs continue to grow in size, its scalability is important. As such, numerous algorithms have been designed for triangle counting-some of which are compute-bound rather than memory bound. Even for compute-bound algorithms, one of the key challenges is the limited control flow available on the processor. This is in-part due to the high dependency between the control flow, input data, and limited utilization of vector instructions. Not surprising, compilers are not always able to detect these data dependencies and vectorize the algorithms. Using the branch-avoiding model we show to remove control flow restrictions by replacing branches with an equivalent set of arithmetic operations. More so, we show how these can be vectorized using Intel's AVX-512 instruction set and that our new vectorized algorithms are 2 × −5× faster than scalar counterparts. We also present a new load balancing method, Logarithmic Radix Binning (LRB) that ensures that threads and the vector data lanes execute a near equal amount of work at any given time. Altogether, our algorithm outperforms several 2017 HPEC Graph Challenge Champions such as the KOKKOS framework and a GPU based algorithm by anywhere from 1.5× and up to 14×.","tags":["Graph Analytics"],"title":"Logarithmic Radix Binning and Vectorized Triangle Counting","type":"publication"},{"authors":["Alok Tripathy","Oded Green"],"categories":[],"content":"","date":1537901014,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1537901014,"objectID":"b3de427adb92266c21674859dee288f1","permalink":"/publication/2018-hpec-sbc/","publishdate":"2018-09-25T14:43:34-04:00","relpermalink":"/publication/2018-hpec-sbc/","section":"publication","summary":"The Betweenness Centrality of a vertex is an important metric used for determining how “central” a vertex is in a graph based on the number of shortest paths going through that vertex. Computing the betweenness centrality of a graph is computationally expensive, O(V ·(V +E)). This has led to the development of several important optimizations includ- ing: approximation, parallelization, and dealing with dynamic updates. Dynamic graph algorithms are extremely favorable as the amount of work that they require is orders of magnitude smaller than their static graph counterparts. Recently, several such dynamic graph algorithms for betweenness centrality have been introduced. Many of these new dynamic graph algorithms tend to have decent parallel scalability when the betweenness centrality metric is computed in an exact manner. However, for the cases where the approximate solution is used, the scalability drops because of bad load-balancing due to the reduction in the amount of work. In this paper, we show a dynamic graph betweenness centrality algorithm that has good parallel scalability for both exact and approximate computations. We show several new optimizations made to the data structures, the load balancing technique, and the parallel granularity that have improved overall performance to 1.6X − 4X faster than one of the fastest previous implementations. More so, our new algorithm scales to larger thread counts than before.","tags":["Graph Analytics"],"title":"Scaling Betweenness Centrality in Dynamic Graphs","type":"publication"}]